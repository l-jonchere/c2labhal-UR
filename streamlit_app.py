import streamlit as st
import pandas as pd
import io
import requests
import json
from metapub import PubMedFetcher
import regex as re
from unidecode import unidecode
from langdetect import detect

# Fonction pour rÃ©cupÃ©rer les donnÃ©es de Scopus
def get_scopus_data(api_key, query, max_items=2000):
    found_items_num = 1
    start_item = 0
    items_per_query = 25
    JSON = []

    while found_items_num > 0:
        resp = requests.get(
            'https://api.elsevier.com/content/search/scopus',
            headers={'Accept': 'application/json', 'X-ELS-APIKey': api_key},
            params={'query': query, 'count': items_per_query, 'start': start_item}
        )

        if resp.status_code != 200:
            raise Exception(f'Scopus API error {resp.status_code}, JSON dump: {resp.json()}')

        if found_items_num == 1:
            found_items_num = int(resp.json().get('search-results').get('opensearch:totalResults'))

        if found_items_num == 0:
            break

        JSON += resp.json()['search-results']['entry']

        start_item += items_per_query
        found_items_num -= items_per_query

        if start_item >= max_items:
            break

    return JSON

# Fonction pour rÃ©cupÃ©rer les donnÃ©es d'OpenAlex
def get_openalex_data(query, max_items=2000):
    url = 'https://api.openalex.org/works'
    params = {'filter': query, 'per-page': 200}
    JSON = []
    next_cursor = None

    while True:
        if next_cursor:
            params['cursor'] = next_cursor

        resp = requests.get(url, params=params)

        if resp.status_code != 200:
            raise Exception(f'OpenAlex API error {resp.status_code}, JSON dump: {resp.json()}')

        data = resp.json()
        JSON += data['results']
        next_cursor = data['meta'].get('next_cursor')

        if not next_cursor or len(JSON) >= max_items:
            break

    return JSON

# Fonction pour rÃ©cupÃ©rer les donnÃ©es de PubMed
def get_pubmed_data(query, max_items=100):
    fetch = PubMedFetcher()
    pmids = fetch.pmids_for_query(query, retmax=max_items)
    data = []

    for pmid in pmids:
        article = fetch.article_by_pmid(pmid)
        pub_date = article.history.get('pubmed', 'N/A')
        if pub_date != 'N/A':
            pub_date = pub_date.date().isoformat()  # Convertir en format YYYY-MM-DD
        data.append({
            'Data source': 'pubmed',
            'Title': article.title,
            'doi': article.doi,
            'id': pmid,
            'Source title': article.journal,
            'Auteurs': article.authors,
            'Date': pub_date
        })
    return data

# Fonction pour convertir les donnÃ©es en DataFrame et ajouter la colonne "source"
def convert_to_dataframe(data, source):
    df = pd.DataFrame(data)
    df['source'] = source
    return df

# Fonction pour nettoyer les DOI
def clean_doi(doi):
    if doi and doi.startswith('https://doi.org/'):
        return doi[len('https://doi.org/'):]
    return doi

# Fonction pour rÃ©cupÃ©rer les donnÃ©es HAL
def safe_get_json(url):
    response = requests.get(url)
    if response.status_code == 200:
        try:
            return response.json()
        except requests.exceptions.JSONDecodeError:
            print(f"Erreur de dÃ©codage JSON pour l'URL : {url}")
            return None
    else:
        print(f"Erreur API : {response.status_code} pour l'URL : {url}")
        return {}

def get_hal_data(collection_a_chercher, start_year, end_year):
    endpoint = "http://api.archives-ouvertes.fr/search/"
    n = safe_get_json(f"{endpoint}{collection_a_chercher}/?q=*&fq=publicationDateY_i:[{start_year} TO {end_year}]&fl=doiId_s,title_s&rows=0&sort=docid asc&wt=json")
    n = n.get('response', {}).get('numFound', 0)
    print(f'Publications trouvÃ©es : {n}')

    dois_coll = []
    titres_coll = []

    if n > 1000:
        current = 0
        cursor = ""
        next_cursor = "*"
        while cursor != next_cursor:
            print(f"\rEn cours : {current}", end="\t")
            cursor = next_cursor
            page = safe_get_json(f"{endpoint}{collection_a_chercher}/?q=*&fq=publicationDateY_i:[{start_year} TO {end_year}]&fl=doiId_s,title_s&rows=1000&cursorMark={cursor}&sort=docid asc&wt=json")
            for d in page.get('response', {}).get('docs', []):
                for t in d.get('title_s', []):
                    titres_coll.append(t)
                dois_coll.append(d.get('doiId_s', '').lower())
            current += 1000
            next_cursor = page.get('nextCursorMark', '')
    else:
        page = safe_get_json(f"{endpoint}{collection_a_chercher}/?q=*&fq=publicationDateY_i:[{start_year} TO {end_year}]&fl=doiId_s,title_s&rows=1000&sort=docid asc&wt=json")
        for d in page.get('response', {}).get('docs', []):
            for t in d.get('title_s', []):
                titres_coll.append(t)
            dois_coll.append(d.get('doiId_s', '').lower())

    print(f"\rTerminÃ© : {n} publications chargÃ©es", end="\t")
    return dois_coll, titres_coll

def normalise(s):
    return re.sub(' +', ' ', unidecode(re.sub(r'\W', ' ', s))).lower()

def inex_match(nti, nti_coll):
    for x in nti_coll:
        if len(nti) * 1.1 > len(x) and len(nti) * 0.9 < len(x):
            if re.fullmatch(f"({nti}){{e<={int(len(x) / 20)}}}", x):
                return True
    return False

def statut_titre(ti, nti_coll):
    if isinstance(ti, str) and ti.endswith("]"):
        try:
            if detect(ti[:re.match(".*\\[", ti).span()[1]]) != detect(ti[re.match(".*\\[", ti).span()[1]:]):
                ti = ti[re.match(".*\\[", ti).span()[1]:]
        except:
            pass
    try:
        nti = normalise(ti)
    except TypeError:
        return "titre invalide"
    if nti in nti_coll:
        return "titre trouvÃ© dans la collection : probablement dÃ©jÃ  prÃ©sent"
    elif inex_match(nti, nti_coll):
        return "titre approchant trouvÃ© dans la collection : Ã  vÃ©rifier"
    else:
        hal_result = safe_get_json(f"http://api.archives-ouvertes.fr/search/?q=title_t:{nti}&rows=0")
        if hal_result.get('response', {}).get('numFound', 0) > 0:
            return "titre trouvÃ© dans HAL mais hors de la collection : affiliation probablement Ã  corriger"
        else:
            return "hors HAL"

def statut_doi(do, dois_coll):
    if pd.isna(do):
        return "pas de DOI valide"
    ndo = do.replace("https://doi.org/", "").lower()
    if ndo in dois_coll:
        return "Dans la collection"
    else:
        hal_result = safe_get_json(f"http://api.archives-ouvertes.fr/search/?q=doiId_id:{ndo}&rows=0")
        if hal_result.get('response', {}).get('numFound', 0) > 0:
            return "Dans HAL mais hors de la collection"
        else:
            return "hors HAL"

# Fonction pour fusionner les lignes en gardant les valeurs identiques et en concatÃ©nant les valeurs diffÃ©rentes
def merge_rows_with_sources(group):
    # Conserver les IDs et les sources sÃ©parÃ©s par un |, et fusionner les autres champs
    if 'id' in group.columns:
        merged_ids = '|'.join(map(str, group['id'].dropna()))
    else:
        merged_ids = None

    merged_sources = '|'.join(group['Data source'].dropna())

    # Initialiser une nouvelle ligne avec les valeurs de la premiÃ¨re ligne
    first_row = group.iloc[0].copy()

    # Pour chaque colonne, vÃ©rifier si les valeurs sont identiques ou diffÃ©rentes
    for column in group.columns:
        if column not in ['id', 'Data source']:
            unique_values = group[column].dropna().apply(lambda x: str(x) if isinstance(x, list) else x).unique()
            if len(unique_values) == 1:
                first_row[column] = unique_values[0]
            else:
                first_row[column] = '|'.join(map(str, unique_values))

    # Mettre Ã  jour les IDs et les sources dans la nouvelle ligne
    first_row['id'] = merged_ids
    first_row['Data source'] = merged_sources

    return first_row

# Fonction pour rÃ©cupÃ©rer les auteurs Ã  partir de Crossref
def get_authors_from_crossref(doi):
    url = f"https://api.crossref.org/works/{doi}"
    response = requests.get(url)

    if response.status_code != 200:
        return []

    data = response.json()
    authors = data.get('message', {}).get('author', [])
    author_names = [author.get('given', '') + ' ' + author.get('family', '') for author in authors]

    return author_names

# Fonction principale
def main():
    st.title("ðŸ¥Ž c2LabHAL")
    st.subheader("Comparez les publications d'un labo dans Scopus, OpenAlex et Pubmed avec sa collection HAL")

    # Saisie des paramÃ¨tres
    collection_a_chercher = st.text_input(
        "Collection HAL",
        value="",
        key="collection_hal",
        help="Saisissez le nom de la collection HAL du laboratoire, par exemple MIP"
    )

    openalex_institution_id = st.text_input("Identifiant OpenAlex du labo", help="Saisissez l'identifiant du labo dans OpenAlex, par exemple i4392021216")
    pubmed_id = st.text_input("RequÃªte PubMed", help="Saisissez la requÃªte Pubmed qui rassemble le mieux les publications du labo, par exemple ((MIP[Affiliation]) AND ((mans[Affiliation]) OR (nantes[Affiliation]))) OR (EA 4334[Affiliation]) OR (EA4334[Affiliation]) OR (UR 4334[Affiliation]) OR (UR4334[Affiliation]) OR (Movement Interactions Performance[Affiliation] OR (MotricitÃ© Interactions Performance[Affiliation]) OR (mouvement interactions performance[Affiliation])")

    col1, col2 = st.columns(2)
    with col1:
        scopus_lab_id = st.text_input("Identifiant Scopus du labo", help="Saisissez le Scopus Affiliation Identifier du laboratoire, par exemple 60105638")
    with col2:
        scopus_api_key = st.text_input("ClÃ© API Scopus", help="Pour obtenir une clÃ© API : https://dev.elsevier.com/. Sinon, contactez la personne en charge de la bibliomÃ©trie dans votre Ã©tablissement")

    start_year = st.number_input("AnnÃ©e de dÃ©but", min_value=1900, max_value=2100, value=2020)
    end_year = st.number_input("AnnÃ©e de fin", min_value=1900, max_value=2100, value=2025)

    # Initialiser la barre de progression
    progress_bar = st.progress(0)
    progress_text = st.empty()

    if st.button("Rechercher"):
        # Initialiser des DataFrames vides
        scopus_df = pd.DataFrame()
        openalex_df = pd.DataFrame()
        pubmed_df = pd.DataFrame()

        # Ã‰tape 1 : RÃ©cupÃ©ration des donnÃ©es OpenAlex
        with st.spinner("OpenAlex"):
            progress_text.text("Ã‰tape 1 : RÃ©cupÃ©ration des donnÃ©es OpenAlex")
            progress_bar.progress(10)
            if openalex_institution_id:
                openalex_query = f"institutions.id:{openalex_institution_id},publication_year:{start_year}-{end_year}"
                openalex_data = get_openalex_data(openalex_query)
                openalex_df = convert_to_dataframe(openalex_data, 'openalex')
                openalex_df['Source title'] = openalex_df.apply(
                    lambda row: row['primary_location']['source']['display_name'] if row['primary_location'] and row['primary_location'].get('source') else None, axis=1
                )
                openalex_df['Date'] = openalex_df.apply(
                    lambda row: row.get('publication_date', None), axis=1
                )
                openalex_df['doi'] = openalex_df.apply(
                    lambda row: row.get('doi', None), axis=1
                )
                openalex_df['id'] = openalex_df.apply(
                    lambda row: row.get('id', None), axis=1
                )
                openalex_df['title'] = openalex_df.apply(
                    lambda row: row.get('title', None), axis=1
                )
                openalex_df = openalex_df[['source', 'title', 'doi', 'id', 'Source title', 'Date']]
                openalex_df.columns = ['Data source', 'Title', 'doi', 'id', 'Source title', 'Date']
                openalex_df['doi'] = openalex_df['doi'].apply(clean_doi)

        # Ã‰tape 2 : RÃ©cupÃ©ration des donnÃ©es PubMed
        with st.spinner("Pubmed"):
            progress_text.text("Ã‰tape 2 : RÃ©cupÃ©ration des donnÃ©es PubMed")
            progress_bar.progress(30)
            if pubmed_id:
                pubmed_query = f"{pubmed_id} AND {start_year}/01/01:{end_year}/12/31[Date - Publication]"
                pubmed_data = get_pubmed_data(pubmed_query)
                pubmed_df = pd.DataFrame(pubmed_data)

        # Ã‰tape 3 : RÃ©cupÃ©ration des donnÃ©es Scopus
        with st.spinner("Scopus"):
            progress_text.text("Ã‰tape 3 : RÃ©cupÃ©ration des donnÃ©es Scopus")
            progress_bar.progress(50)
            if scopus_api_key and scopus_lab_id:
                scopus_query = f"af-ID({scopus_lab_id}) AND PUBYEAR > {start_year - 1} AND PUBYEAR < {end_year + 1}"
                scopus_data = get_scopus_data(scopus_api_key, scopus_query)
                scopus_df = convert_to_dataframe(scopus_data, 'scopus')

                # Enrichir directement avec les auteurs via les EIDs
                scopus_df = enrich_scopus_with_authors(scopus_df, scopus_api_key)

                scopus_df = scopus_df[['source', 'dc:title', 'prism:doi', 'dc:identifier', 'prism:publicationName', 'prism:coverDate', 'Auteurs']]
                scopus_df.columns = ['Data source', 'Title', 'doi', 'id', 'Source title', 'Date', 'Auteurs']

        # Ã‰tape 4 : Comparaison avec HAL
        with st.spinner("HAL"):
            progress_text.text("Ã‰tape 4 : Comparaison avec HAL")
            progress_bar.progress(70)
            # Combiner les DataFrames
            combined_df = pd.concat([scopus_df, openalex_df, pubmed_df], ignore_index=True)

            # RÃ©cupÃ©rer les donnÃ©es HAL
            dois_coll, titres_coll = get_hal_data(collection_a_chercher, start_year, end_year)
            nti_coll = [normalise(x).strip() for x in titres_coll]

            # Ajouter la colonne "Statut"
            combined_df['Statut'] = ''

            for i, r in combined_df.iterrows():
                print(f"\rTraitÃ© : {i} sur {len(combined_df)}", end="\t\t")
                ret_doi = statut_doi(r.doi, dois_coll)
                if ret_doi in ["pas de DOI valide", "hors HAL"]:
                    ret_ti = statut_titre(r.Title, nti_coll)
                    combined_df.loc[i, 'Statut'] = ret_ti
                else:
                    combined_df.loc[i, 'Statut'] = ret_doi

        # Ã‰tape 5 : Fusion des lignes en double
        with st.spinner("Fusion"):
            progress_text.text("Ã‰tape 5 : Fusion des lignes en double")
            progress_bar.progress(90)
            merged_data = combined_df.groupby('doi', as_index=False).apply(merge_rows_with_sources)

        # Ã‰tape 6 : Ajout des auteurs Ã  partir de Crossref
        with st.spinner("Auteurs Crossref"):
            progress_text.text("Ã‰tape 6 : Ajout des auteurs")
            progress_bar.progress(95)
            merged_data['Auteurs'] = merged_data['doi'].apply(lambda doi: '; '.join(get_authors_from_crossref(doi)) if doi else '')

        # GÃ©nÃ©rer le CSV Ã  partir du DataFrame
        csv = merged_data.to_csv(index=False)

        # CrÃ©er un objet BytesIO pour stocker le CSV
        csv_bytes = io.BytesIO()
        csv_bytes.write(csv.encode('utf-8'))
        csv_bytes.seek(0)

        # Proposer le tÃ©lÃ©chargement du CSV
        st.download_button(
            label="TÃ©lÃ©charger le CSV",
            data=csv_bytes,
            file_name="results.csv",
            mime="text/csv"
        )

        # Mettre Ã  jour la barre de progression Ã  100%
        progress_bar.progress(100)
        progress_text.text("TerminÃ© !")

if __name__ == "__main__":
    main()
