import streamlit as st # Utilis√© pour st.error, st.warning, st.info dans certaines fonctions
import pandas as pd
import requests
import json
from metapub import PubMedFetcher
import regex as re
from unidecode import unidecode
import unicodedata
from difflib import get_close_matches
from langdetect import detect
from tqdm import tqdm # Utilis√© pour les barres de progression, notamment avec pandas
from concurrent.futures import ThreadPoolExecutor

# Configurer tqdm pour pandas, cela affecte le comportement global de pandas avec tqdm
tqdm.pandas()

# --- Constantes Partag√©es ---
HAL_API_ENDPOINT = "http://api.archives-ouvertes.fr/search/"
HAL_FIELDS_TO_FETCH = "docid,doiId_s,title_s,submitType_s,linkExtUrl_s,linkExtId_s"
DEFAULT_START_YEAR = 2018
DEFAULT_END_YEAR = '*' # Pour Solr, '*' signifie jusqu'√† la fin/plus r√©cent

# R√®gles d'√©chappement pour les requ√™tes Solr
SOLR_ESCAPE_RULES = {
    '+': r'\+', '-': r'\-', '&': r'\&', '|': r'\|', '!': r'\!', '(': r'\(',
    ')': r'\)', '{': r'\{', '}': r'\}', '[': r'\[', ']': r'\]', '^': r'\^',
    '~': r'\~', '*': r'\*', '?': r'\?', ':': r'\:', '"': r'\"'
}

# --- Fonctions Utilitaires ---

def get_scopus_data(api_key, query, max_items=2000):
    """
    R√©cup√®re les donn√©es de Scopus en fonction d'une requ√™te.
    Args:
        api_key (str): Cl√© API pour Scopus.
        query (str): Requ√™te de recherche Scopus.
        max_items (int): Nombre maximum d'√©l√©ments √† r√©cup√©rer.
    Returns:
        list: Liste des entr√©es Scopus trouv√©es, ou liste vide en cas d'erreur.
    """
    found_items_num = -1 # Initialiser pour que la premi√®re r√©cup√©ration de totalResults se fasse
    start_item = 0
    items_per_query = 25 # Limite de Scopus par requ√™te
    results_json = []
    processed_items = 0

    while True:
        if found_items_num != -1 and (processed_items >= found_items_num or processed_items >= max_items) :
            break # Sortir si tous les items ont √©t√© r√©cup√©r√©s ou si max_items est atteint

        try:
            resp = requests.get(
                'https://api.elsevier.com/content/search/scopus',
                headers={'Accept': 'application/json', 'X-ELS-APIKey': api_key},
                params={'query': query, 'count': items_per_query, 'start': start_item},
                timeout=30 # Timeout pour la requ√™te
            )
            resp.raise_for_status()  # L√®ve une exception pour les codes d'√©tat HTTP 4xx/5xx
            data = resp.json()
        except requests.exceptions.RequestException as e:
            st.error(f"Erreur lors de la requ√™te Scopus (start_item: {start_item}): {e}")
            return results_json # Retourner ce qui a √©t√© collect√© jusqu'√† pr√©sent

        search_results = data.get('search-results', {})
        
        if found_items_num == -1: # Premi√®re requ√™te, r√©cup√©rer le total
            try:
                found_items_num = int(search_results.get('opensearch:totalResults', 0))
                if found_items_num == 0:
                    st.info("Aucun r√©sultat trouv√© sur Scopus pour cette requ√™te.")
                    return []
            except (ValueError, TypeError):
                st.error("R√©ponse inattendue de Scopus (totalResults non trouv√© ou invalide).")
                return []
        
        entries = search_results.get('entry')
        if entries:
            results_json.extend(entries)
            processed_items += len(entries)
        else: # Plus d'entr√©es ou fin des r√©sultats pagin√©s
            if found_items_num > 0 and not entries and start_item < found_items_num :
                 st.warning(f"Scopus: {found_items_num} r√©sultats attendus, mais 'entry' est vide √† start_item {start_item}. Arr√™t.")
            break 

        start_item += items_per_query
        
        if not entries and start_item > found_items_num : # Double s√©curit√© pour sortir
            break

    return results_json[:max_items] # S'assurer de ne pas d√©passer max_items

def get_openalex_data(query, max_items=2000):
    """
    R√©cup√®re les donn√©es d'OpenAlex en fonction d'une requ√™te.
    Args:
        query (str): Requ√™te de filtre OpenAlex.
        max_items (int): Nombre maximum d'√©l√©ments √† r√©cup√©rer.
    Returns:
        list: Liste des travaux OpenAlex trouv√©s, ou liste vide en cas d'erreur.
    """
    url = 'https://api.openalex.org/works'
    # OpenAlex recommande d'inclure un email dans les requ√™tes pour le pool poli
    email = "hal.dbm@listes.u-paris.fr" # Remplacez par un email de contact appropri√©
    params = {'filter': query, 'per-page': 200, 'mailto': email} # Max per-page pour OpenAlex est 200
    results_json = []
    next_cursor = "*" # Initialisation correcte du curseur pour la premi√®re page

    retries = 3 # Nombre de tentatives en cas d'erreur
    
    while len(results_json) < max_items:
        current_try = 0
        if not next_cursor: # Plus de pages √† charger
            break
        
        params['cursor'] = next_cursor

        while current_try < retries:
            try:
                resp = requests.get(url, params=params, timeout=30) # Ajout d'un timeout
                resp.raise_for_status() # L√®ve une exception pour les erreurs HTTP
                data = resp.json()
                
                if 'results' in data:
                    results_json.extend(data['results'])
                
                next_cursor = data.get('meta', {}).get('next_cursor')
                break # Sortir de la boucle de tentatives si succ√®s
            
            except requests.exceptions.RequestException as e:
                current_try += 1
                st.warning(f"Erreur OpenAlex (tentative {current_try}/{retries}): {e}. R√©essai...")
                if current_try >= retries:
                    st.error(f"√âchec de la r√©cup√©ration des donn√©es OpenAlex apr√®s {retries} tentatives.")
                    return results_json[:max_items] # Retourner ce qui a √©t√© collect√©
            except json.JSONDecodeError:
                current_try +=1
                st.warning(f"Erreur de d√©codage JSON OpenAlex (tentative {current_try}/{retries}). R√©essai...")
                if current_try >= retries:
                    st.error("√âchec du d√©codage JSON OpenAlex.")
                    return results_json[:max_items]
        
        if current_try >= retries: # Si toutes les tentatives ont √©chou√©
            break
            
    return results_json[:max_items] # S'assurer de ne pas d√©passer max_items


def get_pubmed_data(query, max_items=1000):
    """
    R√©cup√®re les donn√©es de PubMed pour une requ√™te donn√©e.
    Args:
        query (str): Requ√™te PubMed.
        max_items (int): Nombre maximum d'articles √† r√©cup√©rer.
    Returns:
        list: Liste de dictionnaires, chaque dictionnaire repr√©sentant un article.
    """
    fetch = PubMedFetcher()
    data = []
    try:
        # R√©cup√©rer les PMIDs pour la requ√™te
        pmids = fetch.pmids_for_query(query, retmax=max_items)
        
        # Pour chaque PMID, r√©cup√©rer les d√©tails de l'article
        for pmid in tqdm(pmids, desc="R√©cup√©ration des articles PubMed"):
            try:
                article = fetch.article_by_pmid(pmid)
                # Extraire la date de publication PubMed (si disponible)
                pub_date_obj = article.history.get('pubmed') if article.history else None
                pub_date_str = pub_date_obj.date().isoformat() if pub_date_obj and hasattr(pub_date_obj, 'date') else 'N/A'
                
                data.append({
                    'Data source': 'pubmed',
                    'Title': article.title if article.title else "N/A",
                    'doi': article.doi if article.doi else None,
                    'id': pmid, # L'ID PubMed est le PMID lui-m√™me
                    'Source title': article.journal if article.journal else "N/A", # Nom de la revue
                    'Date': pub_date_str
                })
            except Exception as e_article:
                st.warning(f"Erreur lors de la r√©cup√©ration des d√©tails pour l'article PubMed (PMID: {pmid}): {e_article}")
                # Ajouter une entr√©e partielle m√™me en cas d'erreur pour ne pas perdre le PMID
                data.append({
                    'Data source': 'pubmed', 'Title': "Erreur de r√©cup√©ration", 'doi': None,
                    'id': pmid, 'Source title': "N/A", 'Date': "N/A"
                })
        return data
    except Exception as e_query:
        st.error(f"Erreur lors de la requ√™te PMIDs √† PubMed: {e_query}")
        return [] # Retourner une liste vide en cas d'erreur majeure

def convert_to_dataframe(data, source_name):
    """
    Convertit une liste de dictionnaires en DataFrame pandas et ajoute une colonne 'source'.
    Args:
        data (list): Liste de dictionnaires.
        source_name (str): Nom de la source de donn√©es (ex: 'scopus', 'openalex').
    Returns:
        pd.DataFrame: DataFrame avec les donn√©es et la colonne 'source'.
    """
    if not data: # Si la liste est vide
        return pd.DataFrame() # Retourner un DataFrame vide pour √©viter les erreurs
    df = pd.DataFrame(data)
    df['Data source'] = source_name # Nommer la colonne 'Data source' pour la coh√©rence
    return df

def clean_doi(doi_value):
    """
    Nettoie un DOI en retirant le pr√©fixe 'https://doi.org/'.
    Args:
        doi_value (str or any): La valeur du DOI.
    Returns:
        str or any: Le DOI nettoy√©, ou la valeur originale si non applicable.
    """
    if isinstance(doi_value, str):
        doi_value = doi_value.strip() # Enlever les espaces avant/apr√®s
        if doi_value.startswith('https://doi.org/'):
            return doi_value[len('https://doi.org/'):]
    return doi_value


def escapedSeq(term_char_list):
    """ G√©n√©rateur pour √©chapper les caract√®res Solr. """
    for char in term_char_list:
        yield SOLR_ESCAPE_RULES.get(char, char)

def escapeSolrArg(term_to_escape):
    """
    √âchappe les caract√®res sp√©ciaux Solr dans un terme de requ√™te.
    Args:
        term_to_escape (str): Le terme √† √©chapper.
    Returns:
        str: Le terme avec les caract√®res sp√©ciaux √©chapp√©s.
    """
    if not isinstance(term_to_escape, str):
        return "" # Retourner une cha√Æne vide si l'entr√©e n'est pas une cha√Æne
    # √âchapper l'antislash d'abord, car il est utilis√© dans les s√©quences d'√©chappement
    term_escaped = term_to_escape.replace('\\', r'\\')
    return "".join(list(escapedSeq(term_escaped)))


def normalise(text_to_normalise):
    """
    Normalise une cha√Æne de caract√®res : suppression des accents, conversion en minuscules,
    remplacement des caract√®res non alphanum√©riques par des espaces, et suppression des espaces multiples.
    Args:
        text_to_normalise (str): La cha√Æne √† normaliser.
    Returns:
        str: La cha√Æne normalis√©e.
    """
    if not isinstance(text_to_normalise, str):
        return "" # Retourner une cha√Æne vide si l'entr√©e n'est pas une cha√Æne
    # Supprimer les accents et convertir en ASCII approchant
    text_unaccented = unidecode(text_to_normalise)
    # Remplacer les caract√®res non alphanum√©riques (sauf espaces) par un espace
    text_alphanum_spaces = re.sub(r'[^\w\s]', ' ', text_unaccented)
    # Convertir en minuscules et supprimer les espaces multiples et en d√©but/fin
    text_normalised = re.sub(r'\s+', ' ', text_alphanum_spaces).lower().strip()
    return text_normalised

def compare_inex(norm_title1, norm_title2, threshold_strict=0.9, threshold_short=0.85, short_len_def=20):
    """
    Compare deux titres normalis√©s pour √©valuer leur similarit√©.
    Utilise get_close_matches de difflib.
    Args:
        norm_title1 (str): Premier titre normalis√©.
        norm_title2 (str): Deuxi√®me titre normalis√©.
        threshold_strict (float): Seuil de similarit√© pour les titres plus longs.
        threshold_short (float): Seuil de similarit√© pour les titres courts.
        short_len_def (int): Longueur d√©finissant un titre comme "court".
    Returns:
        bool: True si les titres sont consid√©r√©s similaires, False sinon.
    """
    if not norm_title1 or not norm_title2: # G√©rer les cha√Ænes vides
        return False
    
    # Ajuster le seuil en fonction de la longueur du titre le plus court
    # (pour √™tre plus indulgent avec les titres tr√®s courts o√π une petite diff√©rence p√®se lourd)
    shorter_len = min(len(norm_title1), len(norm_title2))
    current_threshold = threshold_strict if shorter_len > short_len_def else threshold_short
    
    # Comparaison de longueur (optionnelle, get_close_matches g√®re d√©j√† les diff√©rences)
    # if not (len(norm_title1) * 1.25 > len(norm_title2) > len(norm_title1) * 0.75): # Fen√™tre de longueur un peu plus large
    #     return False
        
    matches = get_close_matches(norm_title1, [norm_title2], n=1, cutoff=current_threshold)
    return bool(matches)


def ex_in_coll(original_title_to_check, collection_df):
    """
    V√©rifie si un titre (original, non normalis√©) existe exactement dans la colonne 'Titres' du DataFrame de la collection.
    Args:
        original_title_to_check (str): Le titre original √† rechercher.
        collection_df (pd.DataFrame): DataFrame de la collection HAL (doit contenir 'Titres', 'Hal_ids', etc.).
    Returns:
        list or False: Liste avec le statut et les infos HAL si trouv√©, sinon False.
    """
    if 'Titres' not in collection_df.columns or collection_df.empty:
        return False
    
    # Filtrer pour trouver une correspondance exacte du titre original
    match_df = collection_df[collection_df['Titres'] == original_title_to_check]
    if not match_df.empty:
        row = match_df.iloc[0]
        return [
            "Titre trouv√© dans la collection : probablement d√©j√† pr√©sent",
            original_title_to_check, # Retourner le titre original qui a match√©
            row.get('Hal_ids', ''),
            row.get('Types de d√©p√¥ts', ''),
            row.get('HAL Link', ''),
            row.get('HAL Ext ID', '')
        ]
    return False

def inex_in_coll(normalised_title_to_check, original_title, collection_df):
    """
    V√©rifie si un titre normalis√© a une correspondance approchante dans la colonne 'nti' (titres normalis√©s)
    du DataFrame de la collection.
    Args:
        normalised_title_to_check (str): Le titre normalis√© √† rechercher.
        original_title (str): Le titre original (pour information si une correspondance est trouv√©e).
        collection_df (pd.DataFrame): DataFrame de la collection HAL (doit contenir 'nti', 'Titres', 'Hal_ids', etc.).
    Returns:
        list or False: Liste avec le statut et les infos HAL si trouv√©, sinon False.
    """
    if 'nti' not in collection_df.columns or collection_df.empty:
        return False
        
    # Parcourir les titres normalis√©s de la collection
    for idx, hal_title_norm_from_coll in enumerate(collection_df['nti']):
        if compare_inex(normalised_title_to_check, hal_title_norm_from_coll): # Comparer les titres normalis√©s
            row = collection_df.iloc[idx]
            return [
                "Titre approchant trouv√© dans la collection : √† v√©rifier",
                row.get('Titres', ''), # Retourner le titre original de HAL pour cette correspondance
                row.get('Hal_ids', ''),
                row.get('Types de d√©p√¥ts', ''),
                row.get('HAL Link', ''),
                row.get('HAL Ext ID', '')
            ]
    return False


def in_hal(title_solr_escaped_exact, original_title_to_check):
    """
    Recherche un titre dans l'ensemble de HAL, d'abord de mani√®re exacte, puis approchante.
    Args:
        title_solr_escaped_exact (str): Titre original, √©chapp√© pour une recherche exacte dans Solr (ex: "titre exact").
        original_title_to_check (str): Le titre original (non normalis√©, non √©chapp√©) pour la recherche approchante et la comparaison.
    Returns:
        list: Liste avec le statut et les infos HAL si trouv√©, ou statut "Hors HAL".
    """
    try:
        # 1. Recherche exacte du titre (utilisant le titre original √©chapp√© et entre guillemets pour Solr)
        # Solr g√®re la tokenisation, donc une recherche title_t:"mon titre" est g√©n√©ralement exacte.
        query_exact = f'title_t:({title_solr_escaped_exact})' # title_solr_escaped_exact devrait d√©j√† √™tre "mon titre"
        
        r_exact_req = requests.get(f"{HAL_API_ENDPOINT}?q={query_exact}&rows=1&fl={HAL_FIELDS_TO_FETCH}", timeout=10)
        r_exact_req.raise_for_status()
        r_exact_json = r_exact_req.json()
        
        if r_exact_json.get('response', {}).get('numFound', 0) > 0:
            doc_exact = r_exact_json['response']['docs'][0]
            # V√©rifier si l'un des titres retourn√©s correspond exactement au titre original (sensible √† la casse et ponctuation)
            if any(original_title_to_check == hal_title for hal_title in doc_exact.get('title_s', [])):
                return [
                    "Titre trouv√© dans HAL mais hors de la collection : affiliation probablement √† corriger",
                    doc_exact.get('title_s', [""])[0],
                    doc_exact.get('docid', ''),
                    doc_exact.get('submitType_s', ''),
                    doc_exact.get('linkExtUrl_s', ''),
                    doc_exact.get('linkExtId_s', '')
                ]

        # 2. Si non trouv√© exactement, recherche approchante (Solr est assez bon pour √ßa avec le titre original non √©chapp√©)
        # Utiliser le titre original, Solr g√®re une certaine flexibilit√©. √âchapper les caract√®res sp√©ciaux pour la requ√™te.
        query_approx = f'title_t:({escapeSolrArg(original_title_to_check)})'

        r_approx_req = requests.get(f"{HAL_API_ENDPOINT}?q={query_approx}&rows=1&fl={HAL_FIELDS_TO_FETCH}", timeout=10)
        r_approx_req.raise_for_status()
        r_approx_json = r_approx_req.json()

        if r_approx_json.get('response', {}).get('numFound', 0) > 0:
            doc_approx = r_approx_json['response']['docs'][0]
            # Comparer le titre original (normalis√©) avec les titres retourn√©s (normalis√©s)
            title_orig_norm = normalise(original_title_to_check)
            if any(compare_inex(title_orig_norm, normalise(hal_title)) for hal_title in doc_approx.get('title_s', [])):
                return [
                    "Titre approchant trouv√© dans HAL mais hors de la collection : v√©rifier les affiliations",
                    doc_approx.get('title_s', [""])[0],
                    doc_approx.get('docid', ''),
                    doc_approx.get('submitType_s', ''),
                    doc_approx.get('linkExtUrl_s', ''),
                    doc_approx.get('linkExtId_s', '')
                ]
    except requests.exceptions.RequestException as e:
        st.warning(f"Erreur de requ√™te √† l'API HAL pour le titre '{original_title_to_check}': {e}")
    except (KeyError, IndexError, json.JSONDecodeError) as e_json: # G√©rer les erreurs de structure JSON ou de d√©codage
        st.warning(f"Structure de r√©ponse HAL inattendue ou erreur JSON pour le titre '{original_title_to_check}': {e_json}")
    
    return ["Hors HAL", original_title_to_check, "", "", "", ""] # Retourner le titre original si non trouv√©


def statut_titre(title_to_check, collection_df):
    """
    D√©termine le statut d'un titre par rapport √† une collection HAL et √† l'ensemble de HAL.
    Args:
        title_to_check (str): Le titre √† v√©rifier.
        collection_df (pd.DataFrame): DataFrame de la collection HAL.
    Returns:
        list: Statut et informations associ√©es.
    """
    if not isinstance(title_to_check, str) or not title_to_check.strip():
        return ["Titre invalide", "", "", "", "", ""]

    original_title = title_to_check # Conserver le titre original pour l'affichage et certaines recherches

    # Tentative de nettoyage des titres avec traductions (ex: "[Titre traduit]")
    # Cette logique peut √™tre affin√©e ou rendue optionnelle
    processed_title_for_norm = original_title
    try:
        # Si le titre se termine par ']' et contient une traduction d√©tectable
        if original_title.endswith("]") and '[' in original_title:
            match_bracket = re.match(r"(.*)\[", original_title) # Capturer la partie avant le crochet
            if match_bracket:
                part_before_bracket = match_bracket.group(1).strip()
                # Optionnel: v√©rifier si la partie entre crochets est une langue diff√©rente
                # Pour simplifier, on peut juste prendre la partie avant le crochet
                if part_before_bracket : # S'assurer qu'il y a quelque chose avant le crochet
                    processed_title_for_norm = part_before_bracket
        # Optionnel: g√©rer les cas o√π deux langues sont concat√©n√©es sans crochet (plus complexe)
        # Exemple simple : si les deux moiti√©s du titre sont dans des langues diff√©rentes
        # elif len(original_title) > 30: # √âviter pour les titres tr√®s courts
        #     mid_point = len(original_title) // 2
        #     part1 = original_title[:mid_point].strip()
        #     part2 = original_title[mid_point:].strip()
        #     if part1 and part2 and detect(part1) != detect(part2):
        #         processed_title_for_norm = part1 
    except Exception: # Ignorer les erreurs de d√©tection de langue ou de regex
        processed_title_for_norm = original_title # Revenir au titre original en cas d'erreur de traitement

    title_normalised = normalise(processed_title_for_norm) # Titre normalis√© pour la comparaison approchante
    # Pour la recherche exacte dans Solr, utiliser le titre original, √©chapp√© et entre guillemets
    title_solr_exact_query_str = '\"' + escapeSolrArg(original_title) + '\"'


    # 1. Recherche exacte du titre original dans la collection
    res_ex_coll = ex_in_coll(original_title, collection_df)
    if res_ex_coll:
        return res_ex_coll

    # 2. Recherche approchante (bas√©e sur le titre normalis√©) dans la collection
    res_inex_coll = inex_in_coll(title_normalised, original_title, collection_df)
    if res_inex_coll:
        return res_inex_coll
        
    # 3. Recherche dans tout HAL (exacte avec titre original, puis approchante)
    # in_hal s'attend au titre original √©chapp√© pour l'exact, et au titre original pour l'approchant
    res_hal_global = in_hal(escapeSolrArg(original_title), original_title) # Simplifi√© : Solr g√®re bien les guillemets via escapeSolrArg si besoin.
                                                                            # Ou passer title_solr_exact_query_str pour la partie exacte.
                                                                            # La fonction in_hal a √©t√© modifi√©e pour g√©rer cela.
    return res_hal_global


def statut_doi(doi_to_check, collection_df):
    """
    D√©termine le statut d'un DOI par rapport √† une collection HAL et √† l'ensemble de HAL.
    Args:
        doi_to_check (str): Le DOI √† v√©rifier.
        collection_df (pd.DataFrame): DataFrame de la collection HAL.
    Returns:
        list: Statut et informations associ√©es.
    """
    if pd.isna(doi_to_check) or not str(doi_to_check).strip():
        return ["Pas de DOI valide", "", "", "", "", ""]

    doi_cleaned_lower = str(doi_to_check).lower().strip()
    
    # 1. V√©rifier dans la collection HAL (colonne 'DOIs')
    if 'DOIs' in collection_df.columns and not collection_df.empty:
        # Cr√©er un ensemble de DOIs de la collection pour une recherche rapide (en minuscules)
        dois_coll_set = set(collection_df['DOIs'].dropna().astype(str).str.lower().str.strip())
        if doi_cleaned_lower in dois_coll_set:
            # R√©cup√©rer la ligne correspondante
            match_series = collection_df[collection_df['DOIs'].astype(str).str.lower().str.strip() == doi_cleaned_lower].iloc[0]
            return [
                "Dans la collection",
                match_series.get('Titres', ''), # Titre HAL associ√©
                match_series.get('Hal_ids', ''),
                match_series.get('Types de d√©p√¥ts', ''),
                match_series.get('HAL Link', ''),
                match_series.get('HAL Ext ID', '')
            ]

    # 2. Si non trouv√© dans la collection, chercher dans tout HAL via l'API
    # Nettoyer le DOI pour la recherche Solr (enlever le pr√©fixe HTTP et √©chapper)
    # doiId_s est g√©n√©ralement le DOI sans le pr√©fixe https://doi.org/
    solr_doi_query_val = escapeSolrArg(doi_cleaned_lower.replace("https://doi.org/", ""))
    
    try:
        # Utiliser le champ doiId_s pour une recherche plus cibl√©e du DOI
        r_req = requests.get(f"{HAL_API_ENDPOINT}?q=doiId_s:\"{solr_doi_query_val}\"&rows=1&fl={HAL_FIELDS_TO_FETCH}", timeout=10)
        r_req.raise_for_status()
        r_json = r_req.json()
        
        if r_json.get('response', {}).get('numFound', 0) > 0:
            doc = r_json['response']['docs'][0]
            return [
                "Dans HAL mais hors de la collection",
                doc.get('title_s', [""])[0], # Premier titre trouv√©
                doc.get('docid', ''),
                doc.get('submitType_s', ''),
                doc.get('linkExtUrl_s', ''),
                doc.get('linkExtId_s', '')
            ]
    except requests.exceptions.RequestException as e:
        st.warning(f"Erreur de requ√™te √† l'API HAL pour le DOI '{doi_to_check}': {e}")
    except (KeyError, IndexError, json.JSONDecodeError) as e_json:
        st.warning(f"Structure de r√©ponse HAL inattendue ou erreur JSON pour le DOI '{doi_to_check}': {e_json}")
        
    return ["Hors HAL", "", "", "", "", ""] # Si non trouv√© apr√®s toutes les v√©rifications


def query_upw(doi_value):
    """
    Interroge l'API Unpaywall pour un DOI donn√©.
    Args:
        doi_value (str): Le DOI √† interroger.
    Returns:
        dict: Dictionnaire contenant les informations d'Unpaywall.
    """
    if pd.isna(doi_value) or not str(doi_value).strip():
        return {"Statut Unpaywall": "DOI manquant", "doi_interroge": str(doi_value)}
    
    doi_cleaned = str(doi_value).strip()
    email = "hal.dbm@listes.u-paris.fr" # Email pour l'API Unpaywall (pool poli)
    
    try:
        req = requests.get(f"https://api.unpaywall.org/v2/{doi_cleaned}?email={email}", timeout=15)
        req.raise_for_status()
        res = req.json()
    except requests.exceptions.Timeout:
        return {"Statut Unpaywall": "timeout Unpaywall", "doi_interroge": doi_cleaned}
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            return {"Statut Unpaywall": "non trouv√© dans Unpaywall", "doi_interroge": doi_cleaned}
        return {"Statut Unpaywall": f"erreur HTTP Unpaywall ({e.response.status_code})", "doi_interroge": doi_cleaned}
    except requests.exceptions.RequestException as e:
        return {"Statut Unpaywall": f"erreur requ√™te Unpaywall: {type(e).__name__}", "doi_interroge": doi_cleaned}
    except json.JSONDecodeError:
        return {"Statut Unpaywall": "erreur JSON Unpaywall", "doi_interroge": doi_cleaned}

    # V√©rification suppl√©mentaire si le DOI n'est pas dans Unpaywall
    if res.get("message") and "isn't in Unpaywall" in res.get("message", "").lower():
        return {"Statut Unpaywall": "non trouv√© dans Unpaywall (message API)", "doi_interroge": doi_cleaned}

    # Construction du dictionnaire de r√©sultats
    upw_info = {
        "Statut Unpaywall": "closed" if not res.get("is_oa") else "open",
        "oa_status": res.get("oa_status", ""), 
        "oa_publisher_license": "",
        "oa_publisher_link": "",
        "oa_repo_link": "",
        "publisher": res.get("publisher", ""),
        "doi_interroge": doi_cleaned # Garder une trace du DOI effectivement interrog√©
    }

    # Informations sur la meilleure localisation OA
    best_oa_loc = res.get("best_oa_location")
    if best_oa_loc:
        host_type = best_oa_loc.get("host_type", "")
        license_val = best_oa_loc.get("license") # Peut √™tre None
        url_pdf = best_oa_loc.get("url_for_pdf")
        url_landing = best_oa_loc.get("url") # 'url' est souvent la landing page

        if host_type == "publisher":
            upw_info["oa_publisher_license"] = license_val if license_val else ""
            upw_info["oa_publisher_link"] = url_pdf or url_landing or ""
        elif host_type == "repository":
            upw_info["oa_repo_link"] = str(url_pdf or url_landing or "")

    return upw_info


def enrich_w_upw_parallel(input_df):
    """
    Enrichit un DataFrame avec les donn√©es d'Unpaywall en parall√®le.
    Args:
        input_df (pd.DataFrame): DataFrame d'entr√©e (doit contenir une colonne 'doi').
    Returns:
        pd.DataFrame: DataFrame enrichi avec les colonnes Unpaywall.
    """
    if input_df.empty or 'doi' not in input_df.columns:
        st.warning("DataFrame vide ou colonne 'doi' manquante pour l'enrichissement Unpaywall.")
        # Initialiser les colonnes Unpaywall si elles n'existent pas pour √©viter des erreurs en aval
        upw_cols = ["Statut Unpaywall", "oa_status", "oa_publisher_license", "oa_publisher_link", "oa_repo_link", "publisher", "doi_interroge"]
        for col in upw_cols:
            if col not in input_df.columns:
                input_df[col] = pd.NA
        return input_df

    df_copy = input_df.copy() # Travailler sur une copie
    df_copy.reset_index(drop=True, inplace=True)

    # Extraire les DOIs √† interroger (remplacer NaN par une cha√Æne vide pour √©viter erreur dans query_upw)
    dois_to_query = df_copy['doi'].fillna("").tolist()

    results = []
    # Utiliser ThreadPoolExecutor pour parall√©liser les requ√™tes
    with ThreadPoolExecutor(max_workers=10) as executor: # Ajuster max_workers selon les limites de l'API et les ressources
        # Utiliser tqdm pour une barre de progression (visible en console ou si Streamlit le g√®re)
        results = list(tqdm(executor.map(query_upw, dois_to_query), total=len(dois_to_query), desc="Enrichissement Unpaywall"))

    # Convertir la liste de dictionnaires (r√©sultats) en DataFrame
    if results:
        upw_results_df = pd.DataFrame(results)
        # Fusionner/joindre les r√©sultats avec le DataFrame original copi√©
        # S'assurer que l'ordre est conserv√© ou utiliser une cl√© de jointure si l'index a chang√©
        for col in upw_results_df.columns:
            if col not in df_copy.columns: # Ajouter la colonne si elle n'existe pas
                 df_copy[col] = pd.NA 
            # Assigner les valeurs. S'assurer que la longueur correspond.
            # Si l'ordre est garanti (ce qui est le cas avec map sur une liste), on peut assigner directement.
            df_copy[col] = upw_results_df[col].values 
    else: # Si aucun r√©sultat (ex: tous les DOI √©taient invalides)
        st.info("Aucun r√©sultat d'enrichissement Unpaywall √† ajouter.")
        # S'assurer que les colonnes sont pr√©sentes m√™me si vides
        upw_cols = ["Statut Unpaywall", "oa_status", "oa_publisher_license", "oa_publisher_link", "oa_repo_link", "publisher", "doi_interroge"]
        for col in upw_cols:
            if col not in df_copy.columns:
                df_copy[col] = pd.NA
                
    return df_copy


def add_permissions(row_series_data):
    """
    Ajoute les informations de permission de d√©p√¥t via l'API oa.works (anciennement oadoi.org/sherpa).
    Args:
        row_series_data (pd.Series): Une ligne du DataFrame (repr√©sent√©e comme une Series).
    Returns:
        str: Cha√Æne d√©crivant les conditions de d√©p√¥t, ou un message d'erreur/statut.
    """
    # V√©rifier si un lien de d√©p√¥t OA existe d√©j√† ou si une licence √©diteur claire est pr√©sente
    # Ces informations viennent d'Unpaywall (pr√©c√©demment ajout√©es √† la ligne)
    oa_repo_link_val = str(row_series_data.get("oa_repo_link", "") or "").strip()
    oa_publisher_license_val = str(row_series_data.get("oa_publisher_license", "") or "").strip()

    # Si d√©j√† clairement Open Access via repo ou licence √©diteur, on peut ne pas chercher plus loin
    # ou simplement noter que c'est d√©j√† g√©r√©. Pour l'instant, on continue la recherche pour avoir les infos de oa.works.
    # if oa_repo_link_val or oa_publisher_license_val:
    #     return "D√©j√† OA (repo/licence √©diteur)"

    doi_val = row_series_data.get('doi') # Le DOI original de la publication
    if pd.isna(doi_val) or not str(doi_val).strip():
        return "DOI manquant pour permissions"

    doi_cleaned_for_api = str(doi_val).strip()
    try:
        # Utiliser l'API permissions.oa.works
        req = requests.get(f"https://api.permissions.oa.works/permissions/{doi_cleaned_for_api}", timeout=15)
        # Noter que cette API peut retourner 404 si le DOI n'est pas trouv√©, ce qui est g√©r√© ci-dessous.
        req.raise_for_status() # L√®ve une exception pour les erreurs HTTP 4xx/5xx autres que 404 (si on ne les g√®re pas sp√©cifiquement)
        res_json = req.json()
        
        best_permission_info = res_json.get("best_permission") # Peut √™tre None
        if not best_permission_info:
            return "Aucune permission trouv√©e (oa.works)"

    except requests.exceptions.Timeout:
        return f"Timeout permissions (oa.works) pour DOI {doi_cleaned_for_api}"
    except requests.exceptions.HTTPError as e:
        if e.response.status_code == 404:
            return f"Permissions non trouv√©es (404 oa.works) pour DOI {doi_cleaned_for_api}"
        return f"Erreur HTTP permissions ({e.response.status_code} oa.works) pour DOI {doi_cleaned_for_api}"
    except requests.exceptions.RequestException as e:
        return f"Erreur requ√™te permissions (oa.works) pour DOI {doi_cleaned_for_api}: {type(e).__name__}"
    except json.JSONDecodeError:
        return f"Erreur JSON permissions (oa.works) pour DOI {doi_cleaned_for_api}"

    # Analyser la meilleure permission trouv√©e
    locations_allowed = best_permission_info.get("locations", [])
    # V√©rifier si le d√©p√¥t en "repository" est explicitement autoris√©
    if not any("repository" in str(loc).lower() for loc in locations_allowed):
        return "D√©p√¥t en archive non list√© dans les permissions (oa.works)"

    version_allowed = best_permission_info.get("version", "Version inconnue")
    licence_info = best_permission_info.get("licence", "Licence inconnue")
    embargo_months_val = best_permission_info.get("embargo_months") # Peut √™tre None, 0, ou un entier

    embargo_display_str = "Pas d'embargo sp√©cifi√©"
    if isinstance(embargo_months_val, int):
        if embargo_months_val == 0:
            embargo_display_str = "Pas d'embargo"
        elif embargo_months_val > 0:
            embargo_display_str = f"{embargo_months_val} mois d'embargo"
    
    # Construire la cha√Æne de r√©sultat
    # Privil√©gier les versions "publishedVersion" ou "acceptedVersion"
    if version_allowed.lower() in ["publishedversion", "acceptedversion"]:
        return f"Version autoris√©e (oa.works): {version_allowed} ; Licence: {licence_info} ; Embargo: {embargo_display_str}"
    
    # Si une autre version ou information est disponible
    return f"Info permission (oa.works): {version_allowed} ; {licence_info} ; {embargo_display_str}"


def add_permissions_parallel(input_df):
    """
    Ajoute les informations de permission de d√©p√¥t √† un DataFrame en parall√®le.
    Args:
        input_df (pd.DataFrame): DataFrame d'entr√©e (doit contenir 'doi', et id√©alement les colonnes Unpaywall).
    Returns:
        pd.DataFrame: DataFrame enrichi avec la colonne 'deposit_condition'.
    """
    if input_df.empty or 'doi' not in input_df.columns: # 'doi' est la cl√© pour cette fonction
        st.warning("DataFrame vide ou colonne 'doi' manquante pour l'ajout des permissions.")
        if 'deposit_condition' not in input_df.columns and not input_df.empty:
             input_df['deposit_condition'] = pd.NA # Ajouter la colonne si elle manque
        return input_df

    df_copy = input_df.copy() # Travailler sur une copie
    
    # S'assurer que la colonne 'deposit_condition' existe
    if 'deposit_condition' not in df_copy.columns:
        df_copy['deposit_condition'] = pd.NA

    # Fonction √† appliquer √† chaque ligne (repr√©sent√©e comme une Series pandas)
    def apply_add_permissions_to_row(row_as_series):
        return add_permissions(row_as_series)

    # Utiliser ThreadPoolExecutor pour appliquer la fonction en parall√®le
    # Convertir le DataFrame en une liste de Series (chaque Series est une ligne)
    rows_as_series_list = [row_data for _, row_data in df_copy.iterrows()]
    
    results = []
    with ThreadPoolExecutor(max_workers=10) as executor: # Ajuster max_workers
        results = list(tqdm(executor.map(apply_add_permissions_to_row, rows_as_series_list), total=len(df_copy), desc="Ajout des permissions de d√©p√¥t"))

    if results:
        df_copy['deposit_condition'] = results
    else: # Si aucun r√©sultat (ex: DataFrame d'entr√©e √©tait vide apr√®s filtrage implicite)
        st.info("Aucun r√©sultat d'ajout de permissions.")
        # S'assurer que la colonne existe m√™me si vide
        if 'deposit_condition' not in df_copy.columns:
            df_copy['deposit_condition'] = pd.NA
            
    return df_copy


def deduce_todo(row_data):
    """
    D√©duit les actions √† r√©aliser pour une publication en fonction de son statut HAL, Unpaywall et permissions.
    Args:
        row_data (pd.Series): Une ligne du DataFrame contenant toutes les informations n√©cessaires.
    Returns:
        str: Une cha√Æne de caract√®res d√©crivant les actions sugg√©r√©es, s√©par√©es par " | ".
    """
    # Extraction des informations de la ligne
    statut_hal_val = str(row_data.get("Statut_HAL", "")).strip()
    type_depot_hal_val = str(row_data.get("type_d√©p√¥t_si_trouv√©", "")).strip().lower() # Mettre en minuscule pour la comparaison
    id_hal_val = str(row_data.get("identifiant_hal_si_trouv√©", "")).strip()

    statut_upw_val = str(row_data.get("Statut Unpaywall", "")).strip().lower()
    oa_repo_link_val = str(row_data.get("oa_repo_link", "") or "").strip()
    oa_publisher_link_val = str(row_data.get("oa_publisher_link", "") or "").strip()
    oa_publisher_license_val = str(row_data.get("oa_publisher_license", "") or "").strip()
    deposit_condition_val = str(row_data.get("deposit_condition", "")).lower()

    suggested_actions = []

    # --- Analyse du statut HAL ---
    if statut_hal_val == "Dans la collection" and type_depot_hal_val == "file":
        suggested_actions.append("‚úÖ D√©p√¥t HAL OK (avec fichier).")
    elif statut_hal_val == "Titre trouv√© dans la collection : probablement d√©j√† pr√©sent" and type_depot_hal_val == "file":
        suggested_actions.append("‚úÖ Titre probablement d√©j√† d√©pos√© dans la collection (avec fichier).")
    
    if statut_hal_val == "Dans HAL mais hors de la collection":
        suggested_actions.append("üè∑Ô∏è Affiliation √† v√©rifier dans HAL (trouv√© hors collection).")
    if statut_hal_val == "Titre approchant trouv√© dans HAL mais hors de la collection":
        suggested_actions.append("üîç Titre approchant hors collection. V√©rifier affiliations HAL.")

    if statut_hal_val == "Dans la collection" and type_depot_hal_val != "file" and id_hal_val:
        suggested_actions.append(f"üìÑ Notice HAL ({id_hal_val}) sans fichier. V√©rifier possibilit√© d'ajout de fichier.")
    
    if statut_hal_val in ["Hors HAL", "Titre incorrect, probablement absent de HAL"] and not id_hal_val:
        suggested_actions.append("üì• Cr√©er la notice (et si possible d√©poser le fichier) dans HAL.")
    elif statut_hal_val == "Pas de DOI valide" and not id_hal_val: # Si la recherche par titre (fallback) n'a rien donn√© non plus
        suggested_actions.append("üì• DOI manquant/invalide et titre non trouv√© dans HAL. Cr√©er notice si pertinent.")


    if statut_hal_val == "Titre invalide":
        suggested_actions.append("‚ùå Titre consid√©r√© invalide par le script. V√©rifier/corriger le titre source.")
    if statut_hal_val == "Titre approchant trouv√© dans la collection : √† v√©rifier":
        suggested_actions.append("üßê Titre approchant dans la collection. V√©rifier si c'est une variante d√©j√† d√©pos√©e.")

    # --- Suggestions bas√©es sur Unpaywall et Permissions (si pas d√©j√† "OK avec fichier" dans HAL) ---
    is_hal_ok_with_file = any("‚úÖ D√©p√¥t HAL OK (avec fichier)" in act for act in suggested_actions) or \
                          any("‚úÖ Titre probablement d√©j√† d√©pos√©" in act for act in suggested_actions)

    if not is_hal_ok_with_file:
        # Si OA via un d√©p√¥t en archive (repository) selon Unpaywall
        if oa_repo_link_val:
            suggested_actions.append(f"üîó OA via archive (Unpaywall): {oa_repo_link_val}. Si pas dans HAL, envisager d√©p√¥t notice/fichier.")
        
        # Si OA via √©diteur avec licence selon Unpaywall
        if oa_publisher_link_val and oa_publisher_license_val:
            suggested_actions.append(f"üìú OA √©diteur (licence {oa_publisher_license_val}): {oa_publisher_link_val}. V√©rifier si d√©p√¥t HAL souhait√©/possible.")
        elif oa_publisher_link_val and not oa_publisher_license_val:
             suggested_actions.append(f"üîó OA √©diteur (sans licence claire via UPW): {oa_publisher_link_val}. V√©rifier conditions de d√©p√¥t HAL.")

        # Analyse des conditions de d√©p√¥t (oa.works) si pas d√©j√† clairement OA par ailleurs
        # (Cette condition peut √™tre redondante si oa_repo_link ou oa_publisher_link sont d√©j√† remplis,
        # mais deposit_condition_val peut donner des infos plus pr√©cises sur la version/embargo)
        if "version autoris√©e (oa.works): publishedversion" in deposit_condition_val:
            suggested_actions.append(f"üìÑ D√©p√¥t version √©diteur possible selon oa.works. ({deposit_condition_val})")
        elif "version autoris√©e (oa.works): acceptedversion" in deposit_condition_val:
            suggested_actions.append(f"‚úçÔ∏è D√©p√¥t postprint possible selon oa.works. ({deposit_condition_val})")
        
        # Si ferm√© et aucune condition de d√©p√¥t claire, ou si Unpaywall est "closed"
        if statut_upw_val == "closed" and \
           not ("publishedversion" in deposit_condition_val or "acceptedversion" in deposit_condition_val) and \
           not oa_repo_link_val and not (oa_publisher_link_val and oa_publisher_license_val) : # Si vraiment rien n'indique une ouverture
            suggested_actions.append("üìß Article ferm√© (Unpaywall) et pas de permission claire (oa.works). Contacter auteur pour LRN/d√©p√¥t.")
        
        # Cas o√π Unpaywall ou oa.works retournent des erreurs/statuts non informatifs
        if statut_upw_val not in ["open", "closed", "doi manquant", "non trouv√© dans unpaywall", "non trouv√© dans unpaywall (message api)"] and "erreur" in statut_upw_val: # Ex: timeout, erreur JSON
            suggested_actions.append(f"‚ö†Ô∏è Statut Unpaywall: {statut_upw_val}. V√©rification manuelle des droits n√©cessaire.")
        if "erreur" in deposit_condition_val or "timeout" in deposit_condition_val or ("doi manquant" in deposit_condition_val and not oa_repo_link_val and not oa_publisher_link_val) :
             suggested_actions.append(f"‚ö†Ô∏è Info permissions (oa.works): {deposit_condition_val}. V√©rification manuelle n√©cessaire.")


    if not suggested_actions:
        return "üõ†Ô∏è √Ä v√©rifier manuellement (aucune action sp√©cifique d√©duite)."
        
    # √âviter les doublons d'actions (si diff√©rentes logiques m√®nent √† des suggestions similaires)
    # et joindre. Utiliser un set pour l'unicit√© puis convertir en liste pour l'ordre (si besoin) ou trier.
    return " | ".join(sorted(list(set(suggested_actions))))


def addCaclLinkFormula(pre_url_str, post_url_str, text_for_link):
    """
    Cr√©e une formule de lien hypertexte pour LibreOffice Calc.
    Args:
        pre_url_str (str): Partie initiale de l'URL (ex: "https://hal.science/").
        post_url_str (str): Partie finale de l'URL (ex: "hal-01234567").
        text_for_link (str): Texte √† afficher pour le lien.
    Returns:
        str: Formule HYPERLINK ou cha√Æne vide si les entr√©es sont invalides.
    """
    if post_url_str and text_for_link: # S'assurer que post_url et text ne sont pas None ou vides
        # Nettoyer et s'assurer que ce sont des cha√Ænes
        pre_url_cleaned = str(pre_url_str if pre_url_str else "").strip()
        post_url_cleaned = str(post_url_str).strip()
        text_cleaned = str(text_for_link).strip().replace('"', '""') # √âchapper les guillemets pour la formule

        full_url = f"{pre_url_cleaned}{post_url_cleaned}"
        
        # Tronquer le texte affich√© si trop long
        display_text_final = text_cleaned
        if len(text_cleaned) > 50: # Limite arbitraire pour la lisibilit√©
            display_text_final = text_cleaned[:47] + "..."
            
        return f'=HYPERLINK("{full_url}";"{display_text_final}")'
    return "" # Retourner une cha√Æne vide si pas de lien √† cr√©er


def check_df(input_df_to_check, hal_collection_df, progress_bar_st=None, progress_text_st=None):
    """
    Compare chaque ligne d'un DataFrame d'entr√©e avec les donn√©es d'une collection HAL.
    Args:
        input_df_to_check (pd.DataFrame): DataFrame contenant les publications √† v√©rifier (avec 'doi' et/ou 'Title').
        hal_collection_df (pd.DataFrame): DataFrame de la collection HAL (avec 'DOIs', 'Titres', 'nti', etc.).
        progress_bar_st (st.progress, optional): Barre de progression Streamlit.
        progress_text_st (st.empty, optional): Zone de texte Streamlit pour les messages de progression.
    Returns:
        pd.DataFrame: DataFrame d'entr√©e enrichi avec les colonnes de statut HAL.
    """
    if input_df_to_check.empty:
        st.info("Le DataFrame d'entr√©e pour check_df est vide. Aucune v√©rification HAL √† effectuer.")
        # S'assurer que les colonnes de sortie existent pour √©viter les erreurs en aval
        hal_output_cols = ['Statut_HAL', 'titre_HAL_si_trouv√©', 'identifiant_hal_si_trouv√©', 
                           'type_d√©p√¥t_si_trouv√©', 'HAL Link', 'HAL Ext ID']
        for col_name in hal_output_cols:
            if col_name not in input_df_to_check.columns:
                input_df_to_check[col_name] = pd.NA
        return input_df_to_check

    df_to_process = input_df_to_check.copy() # Travailler sur une copie

    # Initialiser les listes pour stocker les r√©sultats de la comparaison HAL
    statuts_hal_list = []
    titres_hal_list = []
    ids_hal_list = []
    types_depot_hal_list = []
    links_hal_list = []
    ext_ids_hal_list = []

    total_rows_to_process = len(df_to_process)
    # Utiliser tqdm pour la progression (visible en console, Streamlit g√®re sa propre barre)
    for index, row_to_check in tqdm(df_to_process.iterrows(), total=total_rows_to_process, desc="V√©rification HAL (check_df)"):
        doi_value_from_row = row_to_check.get('doi') # Peut √™tre NaN
        title_value_from_row = row_to_check.get('Title') # Peut √™tre NaN ou vide

        # Priorit√© √† la recherche par DOI si disponible et valide
        hal_status_result = ["Pas de DOI valide", "", "", "", "", ""] # Statut par d√©faut
        
        if pd.notna(doi_value_from_row) and str(doi_value_from_row).strip():
            hal_status_result = statut_doi(str(doi_value_from_row), hal_collection_df)
        
        # Si le DOI n'a pas donn√© de r√©sultat concluant (pas trouv√© dans la collection ou HAL global)
        # OU si le DOI √©tait invalide/manquant, alors tenter par titre
        if hal_status_result[0] not in ("Dans la collection", "Dans HAL mais hors de la collection"):
            if pd.notna(title_value_from_row) and str(title_value_from_row).strip():
                # Si la recherche DOI a √©chou√© (ex: "Hors HAL" ou "Pas de DOI valide"),
                # on √©crase son r√©sultat par celui de la recherche par titre.
                hal_status_result = statut_titre(str(title_value_from_row), hal_collection_df)
            elif not (pd.notna(doi_value_from_row) and str(doi_value_from_row).strip()): 
                # Si ni DOI ni Titre valides, marquer comme donn√©es insuffisantes
                hal_status_result = ["Donn√©es d'entr√©e insuffisantes (ni DOI ni Titre)", "", "", "", "", ""]
        
        # Ajouter les r√©sultats aux listes
        statuts_hal_list.append(hal_status_result[0])
        titres_hal_list.append(hal_status_result[1]) # Titre HAL trouv√© ou titre original si non trouv√©
        ids_hal_list.append(hal_status_result[2])
        types_depot_hal_list.append(hal_status_result[3])
        links_hal_list.append(hal_status_result[4])
        ext_ids_hal_list.append(hal_status_result[5])
        
        # Mettre √† jour la barre de progression Streamlit si fournie
        if progress_bar_st is not None and progress_text_st is not None:
            current_progress_val = (index + 1) / total_rows_to_process
            progress_bar_st.progress(int(current_progress_val * 100))
            # Le texte de progression est souvent g√©r√© par l'appelant pour indiquer l'√©tape globale

    # Ajouter les nouvelles colonnes au DataFrame copi√©
    df_to_process['Statut_HAL'] = statuts_hal_list
    df_to_process['titre_HAL_si_trouv√©'] = titres_hal_list
    df_to_process['identifiant_hal_si_trouv√©'] = ids_hal_list
    df_to_process['type_d√©p√¥t_si_trouv√©'] = types_depot_hal_list
    df_to_process['HAL Link'] = links_hal_list
    df_to_process['HAL Ext ID'] = ext_ids_hal_list
    
    if progress_bar_st: progress_bar_st.progress(100) # S'assurer que la barre est √† 100% √† la fin de cette √©tape
    return df_to_process


class HalCollImporter:
    """
    Classe pour importer les donn√©es d'une collection HAL.
    """
    def __init__(self, collection_code: str, start_year_val=None, end_year_val=None):
        self.collection_code = str(collection_code).strip() if collection_code else "" # "" pour tout HAL
        self.start_year = start_year_val if start_year_val is not None else DEFAULT_START_YEAR
        self.end_year = end_year_val if end_year_val is not None else DEFAULT_END_YEAR # '*' pour Solr signifie "jusqu'√† la fin"
        
        self.num_docs_in_collection = self._get_num_docs()

    def _get_num_docs(self):
        """ R√©cup√®re le nombre total de documents dans la collection pour la p√©riode donn√©e. """
        try:
            query_params_count = {
                'q': '*:*', # Interroger tous les documents dans la collection/p√©riode
                'fq': f'publicationDateY_i:[{self.start_year} TO {self.end_year}]',
                'rows': 0, # Ne pas retourner de documents, juste le compte
                'wt': 'json'
            }
            # Construire l'URL de base : si collection_code est vide, interroger tout HAL
            base_search_url = f"{HAL_API_ENDPOINT}{self.collection_code}/" if self.collection_code else HAL_API_ENDPOINT
            
            response_count = requests.get(base_search_url, params=query_params_count, timeout=15)
            response_count.raise_for_status()
            return response_count.json().get('response', {}).get('numFound', 0)
        except requests.exceptions.RequestException as e:
            st.error(f"Erreur API HAL (comptage) pour '{self.collection_code or 'HAL global'}': {e}")
            return 0
        except (KeyError, json.JSONDecodeError):
            st.error(f"R√©ponse API HAL (comptage) inattendue pour '{self.collection_code or 'HAL global'}'.")
            return 0

    def import_data(self):
        """ Importe les donn√©es de la collection HAL pagin√©es. """
        if self.num_docs_in_collection == 0:
            st.info(f"Aucun document trouv√© pour la collection '{self.collection_code or 'HAL global'}' entre {self.start_year} et {self.end_year}.")
            # Retourner un DataFrame vide avec les colonnes attendues pour la coh√©rence
            return pd.DataFrame(columns=['Hal_ids', 'DOIs', 'Titres', 'Types de d√©p√¥ts', 
                                         'HAL Link', 'HAL Ext ID', 'nti'])

        all_docs_list = []
        rows_per_api_page = 1000 # Nombre de documents par page (max pour l'API HAL)
        current_api_cursor = "*" # Curseur initial pour la pagination profonde Solr

        # Construire l'URL de base pour la recherche
        base_search_url = f"{HAL_API_ENDPOINT}{self.collection_code}/" if self.collection_code else HAL_API_ENDPOINT

        # Utiliser tqdm pour la barre de progression
        with tqdm(total=self.num_docs_in_collection, desc=f"Import HAL ({self.collection_code or 'Global'})") as pbar_hal:
            while True:
                query_params_page = {
                    'q': '*:*',
                    'fq': f'publicationDateY_i:[{self.start_year} TO {self.end_year}]',
                    'fl': HAL_FIELDS_TO_FETCH,
                    'rows': rows_per_api_page,
                    'sort': 'docid asc', # Tri n√©cessaire pour la pagination par curseur
                    'cursorMark': current_api_cursor,
                    'wt': 'json'
                }
                try:
                    response_page = requests.get(base_search_url, params=query_params_page, timeout=45) # Timeout plus long pour les grosses requ√™tes
                    response_page.raise_for_status()
                    data_page = response_page.json()
                except requests.exceptions.RequestException as e:
                    st.error(f"Erreur API HAL (import page, curseur {current_api_cursor}): {e}")
                    break # Arr√™ter en cas d'erreur
                except json.JSONDecodeError:
                    st.error(f"Erreur d√©codage JSON (import page HAL, curseur {current_api_cursor}).")
                    break

                docs_on_current_page = data_page.get('response', {}).get('docs', [])
                if not docs_on_current_page: # Plus de documents √† r√©cup√©rer
                    break

                for doc_data in docs_on_current_page:
                    # Un document HAL peut avoir plusieurs titres (ex: langues diff√©rentes)
                    # On cr√©e une entr√©e par titre pour une comparaison plus fine.
                    hal_titles_list = doc_data.get('title_s', [""]) # S'assurer qu'il y a au moins une cha√Æne vide
                    if not isinstance(hal_titles_list, list): hal_titles_list = [str(hal_titles_list)] # Au cas o√π ce ne serait pas une liste

                    for title_item in hal_titles_list:
                        all_docs_list.append({
                            'Hal_ids': doc_data.get('docid', ''),
                            'DOIs': str(doc_data.get('doiId_s', '')).lower() if doc_data.get('doiId_s') else '', # DOI en minuscule, ou cha√Æne vide
                            'Titres': str(title_item), # Titre original de cette entr√©e
                            'Types de d√©p√¥ts': doc_data.get('submitType_s', ''),
                            'HAL Link': doc_data.get('linkExtUrl_s', ''), 
                            'HAL Ext ID': doc_data.get('linkExtId_s', '') 
                        })
                pbar_hal.update(len(docs_on_current_page)) # Mettre √† jour la barre de progression tqdm

                next_api_cursor = data_page.get('nextCursorMark')
                # Condition d'arr√™t de la pagination (si le curseur ne change plus ou est vide)
                if current_api_cursor == next_api_cursor or not next_api_cursor:
                    break
                current_api_cursor = next_api_cursor
        
        if not all_docs_list: # Si aucune donn√©e n'a √©t√© collect√©e malgr√© num_docs > 0
             return pd.DataFrame(columns=['Hal_ids', 'DOIs', 'Titres', 'Types de d√©p√¥ts', 
                                          'HAL Link', 'HAL Ext ID', 'nti'])

        df_collection_hal = pd.DataFrame(all_docs_list)
        # Ajouter la colonne 'nti' (titre normalis√©) pour les comparaisons approchantes
        if 'Titres' in df_collection_hal.columns:
            df_collection_hal['nti'] = df_collection_hal['Titres'].apply(normalise)
        else: # Ne devrait pas arriver si HAL_FIELDS_TO_FETCH inclut title_s
            df_collection_hal['nti'] = ""
            
        return df_collection_hal


def merge_rows_with_sources(grouped_data):
    """
    Fusionne les lignes d'un groupe (par DOI ou Titre) en conservant les informations uniques
    et en concat√©nant les sources de donn√©es et les IDs.
    Args:
        grouped_data (pd.DataFrameGroupBy): Groupe de lignes √† fusionner.
    Returns:
        pd.Series: Une ligne (Series) repr√©sentant les donn√©es fusionn√©es.
    """
    # IDs des sources (ex: ID Scopus, ID OpenAlex, PMID)
    # G√©rer le cas o√π 'id' n'est pas dans les colonnes (ex: donn√©es PubMed seules o√π 'id' est le PMID)
    merged_ids_str = '|'.join(map(str, grouped_data['id'].dropna().astype(str).unique())) if 'id' in grouped_data.columns else None
    
    # Noms des sources de donn√©es (ex: 'scopus|openalex')
    merged_sources_str = '|'.join(grouped_data['Data source'].dropna().astype(str).unique()) if 'Data source' in grouped_data.columns else None

    # Initialiser un dictionnaire pour la nouvelle ligne fusionn√©e
    merged_row_content_dict = {}

    # Parcourir toutes les colonnes du groupe (sauf 'id' et 'Data source' qui sont trait√©es s√©par√©ment)
    for column_name in grouped_data.columns:
        if column_name not in ['id', 'Data source']:
            # Obtenir les valeurs uniques, ignorer les NaN, convertir en str pour la jointure/comparaison
            unique_values_in_col = grouped_data[column_name].dropna().astype(str).unique()
            
            if len(unique_values_in_col) == 1:
                merged_row_content_dict[column_name] = unique_values_in_col[0]
            elif len(unique_values_in_col) > 1:
                # Concat√©ner les valeurs uniques avec '|', tri√©es pour la coh√©rence
                merged_row_content_dict[column_name] = '|'.join(sorted(list(unique_values_in_col)))
            else: # Si toutes les valeurs sont NaN pour cette colonne dans le groupe
                merged_row_content_dict[column_name] = pd.NA # Utiliser pd.NA pour les valeurs manquantes explicites
    
    # Ajouter les champs 'id' et 'Data source' fusionn√©s au dictionnaire
    if merged_ids_str is not None: merged_row_content_dict['id'] = merged_ids_str
    if merged_sources_str is not None: merged_row_content_dict['Data source'] = merged_sources_str
    
    return pd.Series(merged_row_content_dict)


def get_authors_from_crossref(doi_value):
    """
    R√©cup√®re la liste des auteurs pour un DOI donn√© via l'API Crossref.
    Args:
        doi_value (str): Le DOI √† interroger.
    Returns:
        list: Liste des noms d'auteurs (str "Pr√©nom Nom"), ou liste avec message d'erreur.
    """
    if pd.isna(doi_value) or not str(doi_value).strip():
        return ["DOI manquant pour Crossref"]

    doi_cleaned_for_api = str(doi_value).strip()
    # Email recommand√© pour l'API Crossref (pool poli)
    headers = {
        'User-Agent': 'c2LabHAL/1.0 (mailto:YOUR_EMAIL@example.com; https://github.com/GuillaumeGodet/c2labhal)', 
        'Accept': 'application/json'
    }
    url_crossref = f"https://api.crossref.org/works/{doi_cleaned_for_api}"
    
    try:
        response_crossref = requests.get(url_crossref, headers=headers, timeout=10)
        response_crossref.raise_for_status()
        data_crossref = response_crossref.json()
    except requests.exceptions.Timeout:
        return ["Timeout Crossref"]
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code if hasattr(e.response, 'status_code') else 'N/A'
        # if status_code == 404: return [f"DOI non trouv√© sur Crossref ({status_code})"] # Message moins verbeux pour 404
        return [f"Erreur HTTP Crossref ({status_code})"]
    except requests.exceptions.RequestException as e_req:
        return [f"Erreur requ√™te Crossref: {type(e_req).__name__}"]
    except json.JSONDecodeError:
        return ["Erreur JSON Crossref"]

    # Extraction des auteurs
    authors_data_list = data_crossref.get('message', {}).get('author', [])
    if not authors_data_list:
        return [] # Pas d'auteurs trouv√©s ou champ auteur manquant

    author_names_list = []
    for author_entry in authors_data_list:
        if not isinstance(author_entry, dict): continue # Ignorer si l'entr√©e n'est pas un dictionnaire

        given_name = str(author_entry.get('given', '')).strip()
        family_name = str(author_entry.get('family', '')).strip()
        
        full_name = ""
        if given_name and family_name:
            full_name = f"{given_name} {family_name}"
        elif family_name: # Si seulement le nom de famille
            full_name = family_name
        elif given_name: # Si seulement le pr√©nom (rare mais possible)
            full_name = given_name
        
        if full_name: # Ajouter seulement si un nom a √©t√© construit
            author_names_list.append(full_name)

    return author_names_list


def normalize_name(name_to_normalize):
    """
    Normalise un nom d'auteur : minuscules, suppression des accents, gestion des formats "Nom, Pr√©nom".
    Args:
        name_to_normalize (str): Le nom √† normaliser.
    Returns:
        str: Le nom normalis√©.
    """
    if not isinstance(name_to_normalize, str): return ""
    
    name_lower = name_to_normalize.strip().lower()
    # Suppression des accents (NFD normalise en caract√®res de base + diacritiques, puis on filtre les diacritiques)
    name_unaccented = ''.join(c for c in unicodedata.normalize('NFD', name_lower) 
                              if unicodedata.category(c) != 'Mn')
    # Remplacer tirets et points par des espaces, puis normaliser les espaces multiples
    name_cleaned_spaces = name_unaccented.replace('-', ' ').replace('.', ' ')
    name_single_spaced = re.sub(r'\s+', ' ', name_cleaned_spaces).strip()

    # G√©rer les noms au format "Nom, Pr√©nom" (splitter sur la premi√®re virgule seulement)
    if ',' in name_single_spaced:
        parts = [part.strip() for part in name_single_spaced.split(',', 1)]
        if len(parts) == 2 and parts[0] and parts[1]: # S'assurer que les deux parties existent et ne sont pas vides
            # Inverser pour avoir "Pr√©nom Nom"
            return f"{parts[1]} {parts[0]}"
            
    return name_single_spaced


def get_initial_form(normalised_author_name):
    """
    G√©n√®re une forme "Initiale Pr√©nom NomFamille" √† partir d'un nom d√©j√† normalis√©.
    Ex: "jean dupont" -> "j dupont"
    Args:
        normalised_author_name (str): Nom d'auteur normalis√© (ex: "pr√©nom nom").
    Returns:
        str: Forme avec initiale, ou le nom original si mal form√©.
    """
    if not normalised_author_name: return ""
    
    name_parts = normalised_author_name.split()
    if len(name_parts) >= 2: # Au moins un pr√©nom et un nom
        # Prendre la premi√®re lettre du premier mot (pr√©nom) et le dernier mot (nom de famille)
        # Cela g√®re les pr√©noms compos√©s (ex: "Jean-Luc Picard" -> "j picard" si normalis√© en "jean luc picard")
        return f"{name_parts[0][0]} {name_parts[-1]}" 
    elif len(name_parts) == 1: # Si seulement un mot (ex: un seul nom, ou un nom compos√© sans espace apr√®s normalisation)
        return normalised_author_name # Retourner le mot tel quel, car on ne peut pas distinguer pr√©nom/nom
    return "" # Cas o√π le nom normalis√© est vide ou mal form√© apr√®s split
